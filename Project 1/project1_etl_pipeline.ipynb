{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3627d829",
   "metadata": {},
   "source": [
    "# Nathaniel Gleberman - ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a88fed",
   "metadata": {},
   "source": [
    "For this project, I used the following resources to author a segment of an ETL pipeline:\n",
    "- a CSV file from Kaggle for the S&P 500 for 5 years: https://www.kaggle.com/datasets/camnugent/sandp500\n",
    "- another CSV file from Kaggle for Tweets expressing airline sentiment: https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a5f02",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import requests.exceptions\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f888956",
   "metadata": {},
   "source": [
    "#### Declare and Assign Connection Variables for MySQL, MongoDB, & Databases: Change for your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name = \"localhost\"\n",
    "ports = {\"mongo\" : 27017, \"mysql\" : 3306}\n",
    "\n",
    "user_id = \"root\"\n",
    "pwd = \"password123\"\n",
    "\n",
    "src_dbname = \"s&p500_twitter\"\n",
    "dst_dbname = \"stocks_and_tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dae030",
   "metadata": {},
   "source": [
    "#### Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"mongodb://{host_name}:{ports['mongo']}/\"\n",
    "client = pymongo.MongoClient(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f57bc2",
   "metadata": {},
   "source": [
    "#### Define Functions for Getting Data From and Setting Data Into Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    # Create a connection to the MySQL database\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    \n",
    "    #Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.\n",
    "    conn = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, conn);\n",
    "    conn.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(user_id, pwd, host_name, port, db_name, collection, query):\n",
    "    # Create a connection to MongoDB, with or without authentication credentials\n",
    "    if user_id and pwd:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db_name)\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn_str = f\"mongodb://{host_name}:{port}/\"\n",
    "        client = pymongo.MongoClient(conn_str)\n",
    "    \n",
    "    # Query MongoDB, and fill a python list with documents to create a DataFrame \n",
    "    db = client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    # Create a connection to the MySQL database\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    # Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        sqlEngine.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0d1f7",
   "metadata": {},
   "source": [
    "### Dealing with CSV or JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f249e",
   "metadata": {},
   "source": [
    "The following sections deals with CSV or JSON files. The option to convert to or from either exists, but for the sake of this project, the file \"HistoricalQuotes.csv\" is stored in the github repo alongside this page and will be converted to a JSON and used from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9266a",
   "metadata": {},
   "source": [
    "#### Load CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir = os.path.join(os.getcwd())\n",
    "    data_file = os.path.join(data_dir, 'all_stocks_5yr.csv')\n",
    "\n",
    "    df = pd.read_csv(data_file, header=0, index_col=0)\n",
    "    df.head(2)\n",
    "except:\n",
    "    print(\"Error loading all_stocks_5yr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbaebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  \n",
    "    data_dir = os.path.join(os.getcwd())\n",
    "    data_file = os.path.join(data_dir, 'Tweets.csv')\n",
    "\n",
    "    df = pd.read_csv(data_file, header=0, index_col=0)\n",
    "    df.head(2)\n",
    "except:\n",
    "    print(\"Error loading Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afc718",
   "metadata": {},
   "source": [
    "#### Convert CSV into JSON (change file paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below code is adapted from https://pythonexamples.org/python-csv-to-json/\n",
    "def csv_to_json(csvFilePath, jsonFilePath):\n",
    "    jsonArray = []\n",
    "      \n",
    "    #read csv file\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "        #load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csvf) \n",
    "\n",
    "        #convert each csv row into python dict\n",
    "        for row in csvReader: \n",
    "            #add this python dict to json array\n",
    "            jsonArray.append(row)\n",
    "  \n",
    "    #convert python jsonArray to JSON String and write to file\n",
    "    with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "        jsonString = json.dumps(jsonArray, indent=4)\n",
    "        jsonf.write(jsonString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    csvFilePath = r'all_stocks_5yr.csv'\n",
    "    jsonFilePath = r'all_stocks_5yr.json'\n",
    "    csv_to_json(csvFilePath, jsonFilePath)\n",
    "except:\n",
    "    print(\"Error converting all_stocks_5yr.csv to JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    csvFilePath = r'Tweets.csv'\n",
    "    jsonFilePath = r'tweets.json'\n",
    "    csv_to_json(csvFilePath, jsonFilePath)\n",
    "except:\n",
    "    print(\"Error converting Tweets.csv to JSON file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a491512",
   "metadata": {},
   "source": [
    "#### Extra: Convert JSON to CSV (change file paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393825f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(jsonFilePath, csvFilePath):    \n",
    "    #use pandas for conversion\n",
    "    with open(jsonFilePath, encoding='utf-8') as input:\n",
    "        df = pd.read_json(input)\n",
    "    df.to_csv(csvFilePath, encoding='utf-8', index=False)\n",
    "    \n",
    "try:   \n",
    "    csvFilePath = r'all_stocks_5yr.csv'\n",
    "    jsonFilePath = r'all_stocks_5yr.json'\n",
    "    json_to_csv(jsonFilePath, csvFilePath)\n",
    "\n",
    "    csvFilePath = r'Tweets.csv'\n",
    "    jsonFilePath = r'tweets.json'\n",
    "    json_to_csv(jsonFilePath, csvFilePath)\n",
    "except:\n",
    "    print(\"Error concerting from JSON to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2acdf",
   "metadata": {},
   "source": [
    "#### Create the new data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "sqlEngine.execute(f\"DROP DATABASE IF EXISTS `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"CREATE DATABASE `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"USE {dst_dbname};\")\n",
    "\n",
    "db_name = \"stocks_and_tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0b73f",
   "metadata": {},
   "source": [
    "#### Populate MongoDB with source data. DO ONLY ONCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "port = ports[\"mongo\"]\n",
    "conn_str = f\"mongodb://{host_name}:{port}/\"\n",
    "client = pymongo.MongoClient(conn_str)\n",
    "db = client[src_dbname]\n",
    "\n",
    "data_dir = os.path.join(os.getcwd())\n",
    "\n",
    "json_files = {\"twitter_sentiment\" : 'tweets.json',\n",
    "              \"stock_data\" : 'all_stocks_5yr.json'\n",
    "             }\n",
    "\n",
    "for file in json_files:\n",
    "    json_file = os.path.join(data_dir, json_files[file])\n",
    "    with open(json_file, 'r') as openfile:\n",
    "        json_object = json.load(openfile)\n",
    "        file = db[file]\n",
    "        result = file.insert_many(json_object)\n",
    "        #print(f\"{file} was successfully loaded.\")\n",
    "        \n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823878c",
   "metadata": {},
   "source": [
    "### Create and Populate Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf58108",
   "metadata": {},
   "source": [
    "#### 1.1. Extract Data from the Source MongoDB Collections Into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "port = ports[\"mongo\"]\n",
    "collection = \"stock_data\"\n",
    "\n",
    "df_stock_data = get_mongo_dataframe(None, None, host_name, port, src_dbname, collection, query)\n",
    "df_stock_data.head(2)\n",
    "#large data set... please give it a second or two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc99aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {}\n",
    "port = ports[\"mongo\"]\n",
    "collection = \"twitter_sentiment\"\n",
    "\n",
    "df_tweets = get_mongo_dataframe(None, None, host_name, port, src_dbname, collection, query)\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be1b8a",
   "metadata": {},
   "source": [
    "#### Remove/Modify Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a47df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['high','low']\n",
    "df_stock_data.drop(drop_cols, axis=1, inplace=True)\n",
    "df_stock_data.rename(columns={\"Name\":\"ticker\",}, inplace=True)\n",
    "\n",
    "df_stock_data.head(2)\n",
    "#large data set... please give it a second or two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ba18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['tweet_id','name','text','tweet_location','airline_sentiment_gold','negativereason_gold','tweet_coord','user_timezone']\n",
    "df_tweets.drop(drop_cols, axis=1, inplace=True)\n",
    "df_tweets.rename(columns={\"tweet_created\":\"tweet_date\",\n",
    "                         \"airline_sentiment\": \"sentiment\",\n",
    "                         \"airline_sentiment_confidence\":\"sentiment_conf\",\n",
    "                         \"negativereason_confidence\":\"neg_conf\",\n",
    "                         \"negativereason\":\"neg_reason\"\n",
    "                        }, inplace=True)\n",
    "\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a29e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying the date in the tweets dataframe\n",
    "df_tweets['tweet_date'] = pd.to_datetime(df_tweets['tweet_date'], format='%Y-%m-%d', exact=True)\n",
    "df_tweets['tweet_date'] = df_tweets['tweet_date'].dt.strftime('%Y-%m-%d')\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d84b7",
   "metadata": {},
   "source": [
    "#### Add Primary Key Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = 1\n",
    "df_stock_data.insert(loc=0, column='stock_id', value=range(initial_value, len(df_stock_data) +initial_value))\n",
    "df_stock_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = 1\n",
    "df_tweets.insert(loc=0, column='tweet_id', value=range(initial_value, len(df_tweets) +initial_value))\n",
    "df_tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c78cf1",
   "metadata": {},
   "source": [
    "#### Load the Transformed DataFrames into the New Data Warehouse by Creating New Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bfb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_stock_data\n",
    "table_name = 'stock_data'\n",
    "primary_key = 'stock_id'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df_tweets\n",
    "table_name = 'tweet_data'\n",
    "primary_key = 'tweet_id'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95fbec",
   "metadata": {},
   "source": [
    "#### Validate the Tabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_stock_data = \"SELECT * FROM stocks_and_tweets.stock_data;\"\n",
    "df_stocks_data_sql = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_stock_data)\n",
    "df_stocks_data_sql.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_tweet_data = \"SELECT * FROM stocks_and_tweets.tweet_data;\"\n",
    "df_tweet_data_sql = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_tweet_data)\n",
    "df_tweet_data_sql.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b28e1",
   "metadata": {},
   "source": [
    "#### Summary Statistics (Row and column count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records Count for stock data\n",
    "sql_records_stock_count = \"SELECT count(*) AS stock_row_count FROM stocks_and_tweets.stock_data;\"\n",
    "df_records_stock_count = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_records_stock_count)\n",
    "print(df_records_stock_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37c0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records Count for twitter data\n",
    "sql_records_tweet_count = \"SELECT count(*) AS tweet_row_count FROM stocks_and_tweets.tweet_data;\"\n",
    "df_records_tweet_count = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_records_tweet_count)\n",
    "print(df_records_tweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column count for stock data\n",
    "sql_column_stock_count = \"SELECT count(*) AS stock_column_count FROM information_schema.columns WHERE table_name = 'stock_data';\"\n",
    "df_column_stock_count = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_column_stock_count)\n",
    "print(df_column_stock_count)\n",
    "# Reference: https://stackoverflow.com/questions/658395/find-the-number-of-columns-in-a-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57780365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Count for twitter data\n",
    "sql_column_tweet_count = \"SELECT count(*) AS tweet_column_count FROM information_schema.columns WHERE table_name = 'tweet_data';\"\n",
    "df_column_tweet_count = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_column_tweet_count)\n",
    "print(df_column_tweet_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7da854",
   "metadata": {},
   "source": [
    "#### Extra Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_positive_count = \"SELECT COUNT(*) FROM stocks_and_tweets.tweet_data WHERE sentiment='positive';\"\n",
    "df_positive_count = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_positive_count)\n",
    "print(\"The number of tweets with positive sentiment from 2015-02-16 to 2015-02-24 are shown below: \")\n",
    "df_positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ba978",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_negative_count = \"SELECT COUNT(*) FROM stocks_and_tweets.tweet_data WHERE sentiment='negative';\"\n",
    "df_negative_count = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_negative_count)\n",
    "print(\"The number of tweets with negative sentiment from 2015-02-16 to 2015-02-24 are shown below: \")\n",
    "df_negative_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_aal_during_that_time = \"\"\"SELECT open, close, ticker, date\n",
    "                            FROM stocks_and_tweets.stock_data\n",
    "                            WHERE stock_data.ticker = 'AAL' AND\n",
    "                            DATE BETWEEN '2015-02-16' AND '2015-02-24';\"\"\"\n",
    "df_aal_during_that_time = get_sql_dataframe(user_id, pwd, host_name, dst_dbname, sql_aal_during_that_time)\n",
    "print(\"Below is the sql query for American Airlines during that same short time period: \")\n",
    "df_aal_during_that_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1efac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
